<!DOCTYPE html>
<html>
<head>
  <style>
    figcaption {
      background-color: white;
      color: gray;
      font-style: italic;
      padding: 3px;
      text-align: center;
    }
</style>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="In-Context Clustering with Large Language Models"/>
  <meta property="og:description" content="In-Context Clustering (ICC), a flexible LLM-based procedure for clustering data from diverse distributions."/>
  <meta property="og:url" content="https://agenticlearning.ai/icc/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/image_clustering.pdf" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="In-Context Clustering with Large Language Models">
  <meta name="twitter:description" content="In-Context Clustering (ICC), a flexible LLM-based procedure for clustering data from diverse distributions.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/media/image_clustering.pdf">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM,Clustering,Attention,Multimodal">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>In-Context Clustering</title>
  <link rel="icon" type="image/x-icon" href="static/media/icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">In-Context Clustering with Large Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yingwangg.github.io/" target="_blank">Ying Wang</a>,</span>
                <span class="author-block">
                  <a href="https://mengyeren.com/" target="_blank">Mengye Ren</a>,</span>
                  <span class="author-block">
                    <a href="https://cims.nyu.edu/~andrewgw/" target="_blank">Andrew Gordon Wilson</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">New York University</span>
                    <!-- <span class="author-block">New York University<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Agentic-Learning-AI-Lab/icc" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose In-Context Clustering (ICC), a flexible LLM-based procedure for clustering data from diverse distributions. Unlike traditional clustering algorithms constrained by predefined similarity measures, ICC flexibly captures complex relationships among inputs through the attention mechanism. We show that pretrained LLMs exhibit impressive zero-shot clustering capabilities on text-encoded numeric data, with attention matrices showing salient cluster patterns. Spectral clustering using these attention matrices provides surprisingly competitive performance. We further enhance the clustering capabilities of LLMs on numeric and image data through fine-tuning using Next Token Prediction (NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned image clustering, a capability that classical clustering methods lack. Our work extends in-context learning to an unsupervised setting, showcasing the effectiveness and flexibility of LLMs for clustering.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
  <div class="container is-max-desktop">
  
    <div class="hero-body">
      <img src="static/images/icc.jpeg" alt="ICL" class="center"/>
    </div>
    <h2> Different from previous in-context supervised learning that requires multiple input-output pairs in the prompt, ICC extends in-context learning to an unsupervised setting where only unlabeled input data appears in the context. </h2>
  </div>
</section>

<!--Pipeline -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-5"><br>Zero-shot ICC</h2>
    <h2> <b>LLMs pre-trained on large text corpus are capable of zero-shot clustering</b>. The figure below shows zero-shot clustering accuracy of various pretrained LLMs on t-Distribution with different degrees of freedom (df). When df is small, the data distribution has a heavy tail, which violates the Gaussian assumption of k-means. LLMs (especially those with larger model sizes) show impressive zero-shot clustering capabilities on heavy-tailed data.</h2>
    <div class="hero-body">
      <img src="static/images/numeric_zeroshot.jpeg" alt="Zero-shot Clustering"/>
      <figcaption> Figure 1: Zero-Shot Clustering Accuracy. </figcaption>
    </div>
  </div>
</section>
<!-- Pipeline -->

<!--Pipeline -->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <h2> To better understand the inner mechanism of ICC, we visualize the attention scores across different transformer layers. We observe that <b>attention matrices in intermediate layers show block structures that align with cluster identities</b>. Spectral clustering using attention scores yields competitive performance compared to direct LLM generation. This surprising result suggests that attention of LLMs already encodes rich structural information beyond what is directly generated. Please refer to Sec3.2 in our paper for more details.
    <div class="hero-body">
      <img src="static/images/attention.jpeg" alt="Attention Allocation"/>
      <figcaption> Figure 2: Visualization of Attention Allocation on Input Data and Corresponding Cluster Labels. The x-axis and y-axis are the ground-truth cluster labels. The top right curves are the average accuracy of spectral clustering using the input-input attention score matrices (top-left) across different layers, compared with the average accuracy of LLM generation.</figcaption>
    </div>
  </div>
</section>
<!-- Pipeline -->

<section class="hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-5"><br>Improve ICC through Finetuning</h2>
    <h2> While pretrained LLMs show promising zero-shot clustering capabilities, small open-sourced models lag
behind classical methods and proprietary LLMs. We create sythetic clustering data and use simple LoRA fine-tuning with NTP loss to further improve ICC.  </h2>
    <div class="hero-body">
      <img src="static/images/numeric_finetune.jpeg" alt="Effect of Finetuning"/>
      <figcaption> Figure 3: Effect of Finetuning. </figcaption>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="container is-max-desktop">
    <h2> We also extend ICC to multimodal LLMs. By projecting image embeddings obtained from a pretrained visual encoder to language embedding space, LLMs can learn to produce meaningful groupings of images bease their semantic meaning.</h2>
    <div class="hero-body">
      <img src="static/images/image_clustering.jpeg" alt="Image Clustering"/>
      <figcaption> Figure 4: Left: Multimodal LLM Achitecture with Average Pooling for Image Featrues. Right: Qualitative Comparison of Models on Image Clustering — ICC outperforms k-means when the data has rich sematic information </figcaption>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-5"><br>Text-Conditioned CLustering</h2>
    <h2> Real-world data can have multiple plausible clusterings depending on the objective. For example, the same set of animal images can be clustered by visual properties like colors (orange vs. white) or semantic categories like species (dog vs. cat). When the clustering condition changes, classical methods typically require retraining or re-engineering features. In contrast, <b> LLMs can easily adapt to new conditions through prompting thanks to their powerful contextual understanding capability</b>.</h2>
    <div class="hero-body">
      <img src="static/images/condcluster.jpeg" alt="Conditional Image Clustering"/>
      <figcaption> Figure 5: Clustering changes when the condition changes. </figcaption>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
